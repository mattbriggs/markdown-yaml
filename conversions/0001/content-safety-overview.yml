# yaml-language-server: $schema=article.schema.v1v1.json
metadata:
  title: Content Safety in Azure AI Foundry portal overview
  titleSuffix: Azure AI Foundry
  description: Learn how to use Azure AI Content Safety in Azure AI Foundry portal
    to detect harmful user-generated and AI-generated content in applications and
    services.
  manager: nitinme
  ms.service: azure-ai-foundry
  ms.custom: ''
  ms.topic: overview
  ms.date: 02/20/2025
  ms.author: pafarley
  author: PatrickFarley
contents:
- title: Content Safety in the Azure AI Foundry portal
  type: introduction
  summary: Azure AI Content Safety is an AI service that detects harmful user-generated
    and AI-generated content in applications and services. Azure AI Content Safety
    includes various APIs that allow you to detect and prevent the output of harmful
    content. The interactive Content Safety **try out** page in [Azure AI Foundry
    portal](https://ai.azure.com) allows you to view, explore, and try out sample
    code for detecting harmful content across different modalities.
- title: Features
  type: unknown
  summary:
  - 'You can use Azure AI Content Safety for many scenarios:'
  - '**Text content**:'
  - "- Jailbreak Attacks: Attempts by users to manipulate the AI into bypassing its\
    \ safety protocols or ethical guidelines. Examples include prompts designed to\
    \ trick the AI into giving inappropriate responses or performing tasks it was\
    \ programmed to avoid. \n    - Indirect Attacks: Also known as Cross-Domain Prompt\
    \ Injection Attacks, indirect attacks involve embedding malicious prompts within\
    \ documents that the AI might process. For example, if a document contains hidden\
    \ instructions, the AI might inadvertently follow them, leading to unintended\
    \ or unsafe outputs."
  - '**Image content**:'
  - '**Customize your own categories**:'
  - |-
    ---
    title: include file
    description: include file
    ms.author: pafarley
    ms.service: azure-ai-foundry
    ms.topic: include
    ms.date: 01/28/2025
    ms.custom: include
    ---
  listUnordered:
  - item: 'Moderate text content: This feature scans and moderates text content, identifying
      and categorizing it based on different levels of severity to ensure appropriate
      responses.'
  - item: 'Groundedness detection: This filter determines if the AI''s responses are
      based on trusted, user-provided sources, ensuring that the answers are "grounded"
      in the intended material. Groundedness detection is helpful for improving the
      reliability and factual accuracy of responses.'
  - item: 'Protected material detection for text: This feature identifies protected
      text material, such as known song lyrics, articles, or other content, ensuring
      that the AI doesn''t output this content without permission.'
  - item: 'Protected material detection for code: Detects code segments in the model''s
      output that match known code from public repositories, helping to prevent uncredited
      or unauthorized reproduction of source code.'
  - item: 'Prompt shields: This feature provides a unified API to address "Jailbreak"
      and "Indirect Attacks":'
  - - item: 'Moderate image content: Similar to text moderation, this feature filters
        and assesses image content to detect inappropriate or harmful visuals.'
    - item: 'Moderate multimodal content: This is designed to handle a combination
        of text and images, assessing the overall context and any potential risks
        across multiple types of content.'
  - - item: 'Custom categories: Allows users to define specific categories for moderating
        and filtering content, tailoring safety protocols to unique needs.'
    - item: 'Safety system message: Provides a method for setting up a "System Message"
        to instruct the AI on desired behavior and limitations, reinforcing safety
        boundaries and helping prevent unwanted outputs.'
- title: Understand harm categories
  type: unknown
  summary:
  - '### Harm categories'
  - '### Severity levels'
  table:
  - |-
    | Category  | Description         |API term |
    | --------- | ------------------- | --- |
    | Hate and Fairness      | Hate and fairness harms refer to any content that attacks or uses discriminatory language with reference to a person or identity group based on certain differentiating attributes of these groups. <br><br>This includes, but isn't limited to:<ul><li>Race, ethnicity, nationality</li><li>Gender identity groups and expression</li><li>Sexual orientation</li><li>Religion</li><li>Personal appearance and body size</li><li>Disability status</li><li>Harassment and bullying</li></ul> | `Hate` |
    | Sexual  | Sexual describes language related to anatomical organs and genitals, romantic relationships and sexual acts, acts portrayed in erotic or affectionate terms, including those portrayed as an assault or a forced sexual violent act against one's will. <br><br> This includes but isn't limited to:<ul><li>Vulgar content</li><li>Prostitution</li><li>Nudity and Pornography</li><li>Abuse</li><li>Child exploitation, child abuse, child grooming</li></ul>   | `Sexual` |
    | Violence  | Violence describes language related to physical actions intended to hurt, injure, damage, or kill someone or something; describes weapons, guns, and related entities. <br><br>This includes, but isn't limited to:  <ul><li>Weapons</li><li>Bullying and intimidation</li><li>Terrorist and violent extremism</li><li>Stalking</li></ul>  | `Violence` |
    | Self-Harm  | Self-harm describes language related to physical actions intended to purposely hurt, injure, damage one's body or kill oneself. <br><br> This includes, but isn't limited to: <ul><li>Eating Disorders</li><li>Bullying and intimidation</li></ul>  | `SelfHarm` |
  - "| Level | Description |\n| --- | ---|\n|Safe |Content might be related to violence,\
    \ self-harm, sexual, or hate categories. However, the terms are used in general,\
    \ journalistic, scientific, medical, and similar professional contexts, which\
    \ are appropriate for most audiences. |\n|Low |Content that expresses prejudiced,\
    \ judgmental, or opinionated views, includes offensive use of language, stereotyping,\
    \ use-cases exploring a fictional world (for example, gaming, literature) and\
    \ depictions at low intensity.| \n|Medium |Content that uses offensive, insulting,\
    \ mocking, intimidating, or demeaning language towards specific identity groups,\
    \ includes depictions of seeking and executing harmful instructions, fantasies,\
    \ glorification, promotion of harm at medium intensity. |\n|High |Content that\
    \ displays explicit and severe harmful instructions, actions, damage, or abuse;\
    \ includes endorsement, glorification, or promotion of severe harmful acts, extreme\
    \ or illegal forms of harm, radicalization, or nonconsensual power exchange or\
    \ abuse. |"
- title: Limitations
  type: unknown
  summary: Refer to the [Content Safety overview](/azure/ai-services/content-safety/overview)
    for supported regions, rate limits, and input requirements for all features. Refer
    to the [Language support](/azure/ai-services/content-safety/language-support)
    page for supported languages.
- title: Next step
  type: unknown
  summary: Get started using Azure AI Content Safety in [Azure AI Foundry portal](https://ai.azure.com)
    by following the [How-to guide](/azure/ai-services/content-safety/how-to/foundry).
