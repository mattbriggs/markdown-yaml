# yaml-language-server: $schema=../article.schema.v1v1.full.json
'@context': https://schema.org
'@type': Article
headline: Content Safety in the Azure AI Foundry portal
articleBody: "Azure AI Content Safety is an AI service that detects harmful user-generated\
  \ and AI-generated content in applications and services. Azure AI Content Safety\
  \ includes various APIs that allow you to detect and prevent the output of harmful\
  \ content. The interactive Content Safety **try out** page in [Azure AI Foundry\
  \ portal](https://ai.azure.com) allows you to view, explore, and try out sample\
  \ code for detecting harmful content across different modalities. \n\n## Features\n\
  \nYou can use Azure AI Content Safety for many scenarios: \n\n**Text content**:\
  \ \n- Moderate text content: This feature scans and moderates text content, identifying\
  \ and categorizing it based on different levels of severity to ensure appropriate\
  \ responses. \n- Groundedness detection: This filter determines if the AI's responses\
  \ are based on trusted, user-provided sources, ensuring that the answers are \"\
  grounded\" in the intended material. Groundedness detection is helpful for improving\
  \ the reliability and factual accuracy of responses. \n- Protected material detection\
  \ for text: This feature identifies protected text material, such as known song\
  \ lyrics, articles, or other content, ensuring that the AI doesn't output this content\
  \ without permission. \n- Protected material detection for code: Detects code segments\
  \ in the model's output that match known code from public repositories, helping\
  \ to prevent uncredited or unauthorized reproduction of source code. \n- Prompt\
  \ shields: This feature provides a unified API to address \"Jailbreak\" and \"Indirect\
  \ Attacks\": \n    - Jailbreak Attacks: Attempts by users to manipulate the AI into\
  \ bypassing its safety protocols or ethical guidelines. Examples include prompts\
  \ designed to trick the AI into giving inappropriate responses or performing tasks\
  \ it was programmed to avoid. \n    - Indirect Attacks: Also known as Cross-Domain\
  \ Prompt Injection Attacks, indirect attacks involve embedding malicious prompts\
  \ within documents that the AI might process. For example, if a document contains\
  \ hidden instructions, the AI might inadvertently follow them, leading to unintended\
  \ or unsafe outputs. \n\n**Image content**: \n- Moderate image content: Similar\
  \ to text moderation, this feature filters and assesses image content to detect\
  \ inappropriate or harmful visuals. \n- Moderate multimodal content: This is designed\
  \ to handle a combination of text and images, assessing the overall context and\
  \ any potential risks across multiple types of content. \n\n**Customize your own\
  \ categories**: \n- Custom categories: Allows users to define specific categories\
  \ for moderating and filtering content, tailoring safety protocols to unique needs.\
  \ \n- Safety system message: Provides a method for setting up a \"System Message\"\
  \ to instruct the AI on desired behavior and limitations, reinforcing safety boundaries\
  \ and helping prevent unwanted outputs. \n\n---\ntitle: include file\ndescription:\
  \ include file\nms.author: pafarley\nms.service: azure-ai-foundry\nms.topic: include\n\
  ms.date: 01/28/2025\nms.custom: include\n---\n\n## Understand harm categories\n\n\
  ### Harm categories\n\n| Category  | Description         |API term |\n| ---------\
  \ | ------------------- | --- |\n| Hate and Fairness      | Hate and fairness harms\
  \ refer to any content that attacks or uses discriminatory language with reference\
  \ to a person or identity group based on certain differentiating attributes of these\
  \ groups. <br><br>This includes, but isn't limited to:<ul><li>Race, ethnicity, nationality</li><li>Gender\
  \ identity groups and expression</li><li>Sexual orientation</li><li>Religion</li><li>Personal\
  \ appearance and body size</li><li>Disability status</li><li>Harassment and bullying</li></ul>\
  \ | `Hate` |\n| Sexual  | Sexual describes language related to anatomical organs\
  \ and genitals, romantic relationships and sexual acts, acts portrayed in erotic\
  \ or affectionate terms, including those portrayed as an assault or a forced sexual\
  \ violent act against one's will. <br><br> This includes but isn't limited to:<ul><li>Vulgar\
  \ content</li><li>Prostitution</li><li>Nudity and Pornography</li><li>Abuse</li><li>Child\
  \ exploitation, child abuse, child grooming</li></ul>   | `Sexual` |\n| Violence\
  \  | Violence describes language related to physical actions intended to hurt, injure,\
  \ damage, or kill someone or something; describes weapons, guns, and related entities.\
  \ <br><br>This includes, but isn't limited to:  <ul><li>Weapons</li><li>Bullying\
  \ and intimidation</li><li>Terrorist and violent extremism</li><li>Stalking</li></ul>\
  \  | `Violence` |\n| Self-Harm  | Self-harm describes language related to physical\
  \ actions intended to purposely hurt, injure, damage one's body or kill oneself.\
  \ <br><br> This includes, but isn't limited to: <ul><li>Eating Disorders</li><li>Bullying\
  \ and intimidation</li></ul>  | `SelfHarm` |\n\n### Severity levels \n\n| Level\
  \ | Description |\n| --- | ---|\n|Safe |Content might be related to violence, self-harm,\
  \ sexual, or hate categories. However, the terms are used in general, journalistic,\
  \ scientific, medical, and similar professional contexts, which are appropriate\
  \ for most audiences. |\n|Low |Content that expresses prejudiced, judgmental, or\
  \ opinionated views, includes offensive use of language, stereotyping, use-cases\
  \ exploring a fictional world (for example, gaming, literature) and depictions at\
  \ low intensity.| \n|Medium |Content that uses offensive, insulting, mocking, intimidating,\
  \ or demeaning language towards specific identity groups, includes depictions of\
  \ seeking and executing harmful instructions, fantasies, glorification, promotion\
  \ of harm at medium intensity. |\n|High |Content that displays explicit and severe\
  \ harmful instructions, actions, damage, or abuse; includes endorsement, glorification,\
  \ or promotion of severe harmful acts, extreme or illegal forms of harm, radicalization,\
  \ or nonconsensual power exchange or abuse. |\n\n## Limitations\n\nRefer to the\
  \ [Content Safety overview](/azure/ai-services/content-safety/overview) for supported\
  \ regions, rate limits, and input requirements for all features. Refer to the [Language\
  \ support](/azure/ai-services/content-safety/language-support) page for supported\
  \ languages. \n\n\n## Next step\n\nGet started using Azure AI Content Safety in\
  \ [Azure AI Foundry portal](https://ai.azure.com) by following the [How-to guide](/azure/ai-services/content-safety/how-to/foundry)."
datePublished: '2025-04-01T16:34:21.923039'
author:
  name: PatrickFarley
keywords: []
description: Learn how to use Azure AI Content Safety in Azure AI Foundry portal to
  detect harmful user-generated and AI-generated content in applications and services.
inLanguage: en
url: ''
identifier: ''
